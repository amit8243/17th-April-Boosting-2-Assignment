{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a4f1f-0264-49f5-96fc-ceb1a5df1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting-2 Assignment\n",
    "\"\"\"What is Gradient Boosting Regression?\"\"\"\n",
    "\n",
    "Ans: Gradient Boosting Regression, often referred to as Gradient Boosting Machines (GBM), is a powerful \n",
    "machine learning technique that combines multiple weak regression models to create a strong regression\n",
    "model. It is an extension of the Gradient Boosting framework applied to regression problems. GBM is \n",
    "known for its high predictive accuracy and ability to handle complex non-linear relationships between \n",
    "variables. Here's an explanation of how Gradient Boosting Regression works:\n",
    "\n",
    "Initialize the model: Initially, the model starts with a single weak regression model, often a decision \n",
    "tree with low depth. This initial model serves as the base model.\n",
    "\n",
    "Calculate the initial predictions: The base model makes predictions on the training data. These initial\n",
    "predictions are often simple estimates, such as the mean of the target variable.\n",
    "\n",
    "Calculate the residuals: The difference between the actual target values and the initial predictions is\n",
    "calculated. These residuals represent the errors made by the base model.\n",
    "\n",
    "Train a new weak regression model: A new weak regression model is trained to predict the residuals from \n",
    "the previous step. The goal of this new model is to capture the patterns and relationships in the \n",
    "residuals that the base model did not account for. The weak model is typically trained using gradient \n",
    "descent optimization, minimizing the mean squared error (MSE) between the predictions and the residuals.\n",
    "\n",
    "Update the model: The new weak model's predictions are combined with the predictions from the base\n",
    "model to update the overall model's predictions. This update is performed by adding the predictions of \n",
    "the new model to the predictions of the previous models.\n",
    "\n",
    "Repeat steps 3-5: The process of calculating residuals, training a new weak model, and updating the \n",
    "overall model's predictions is repeated for a specified number of iterations. Each iteration focuses on \n",
    "capturing the remaining patterns and residuals that the previous models could not account for.\n",
    "\n",
    "Final prediction: The final prediction of the Gradient Boosting Regression model is the sum of the \n",
    "predictions from all the weak models. This combined prediction represents the strong regression model \n",
    "created by iteratively boosting the performance of weak models.\n",
    "\n",
    "The key idea behind Gradient Boosting Regression is that each weak model in the ensemble is trained to\n",
    "minimize the residuals of the previous models. By iteratively updating the predictions and focusing on\n",
    "the remaining errors, the model gradually improves and becomes more accurate. The gradients of the loss\n",
    "function (e.g., MSE) are used to guide the learning process, which is why it is called \"Gradient\" \n",
    "Boosting.\n",
    "\n",
    "Gradient Boosting Regression is widely used in various domains for regression tasks, and it can handle \n",
    "both numerical and categorical features. It offers flexibility in terms of the choice of weak learners \n",
    "and the loss function to optimize. Regularization techniques, such as controlling the depth of the weak\n",
    "models or using shrinkage/learning rate, can be applied to prevent overfitting and improve \n",
    "generalization.\n",
    "\n",
    "\"\"\"Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\"\"\"\n",
    "\n",
    "Ans: here is the code for a simple gradient boosting algorithm implemented from scratch using Python \n",
    "and NumPy:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_boosting(X, y, n_estimators=100, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  Gradient boosting algorithm for regression.\n",
    "\n",
    "  Args:\n",
    "    X: Training data.\n",
    "    y: Training labels.\n",
    "    n_estimators: Number of trees in the ensemble.\n",
    "    learning_rate: Learning rate.\n",
    "\n",
    "  Returns:\n",
    "    A trained gradient boosting model.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize the model.\n",
    "  model = []\n",
    "\n",
    "  # Train the model.\n",
    "  for i in range(n_estimators):\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X, y - np.array([model[-1].predict(X)] * len(y)))\n",
    "    model.append(tree)\n",
    "\n",
    "  # Return the model.\n",
    "  return model\n",
    "\n",
    "def main():\n",
    "  # Load the data.\n",
    "  X, y = np.loadtxt('data.csv', delimiter=',', unpack=True)\n",
    "\n",
    "  # Train the model.\n",
    "  model = gradient_boosting(X, y)\n",
    "\n",
    "  # Evaluate the model.\n",
    "  predictions = np.array([model[-1].predict(X)] * len(y))\n",
    "  mean_squared_error = np.mean((predictions - y)**2)\n",
    "  r_squared = np.corrcoef(predictions, y)[0, 1]**2\n",
    "\n",
    "  # Print the results.\n",
    "  print('Mean squared error:', mean_squared_error)\n",
    "  print('R-squared:', r_squared)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "\n",
    "Here is a breakdown of the code:\n",
    "\n",
    "The gradient_boosting() function takes the training data, training labels, number of trees, and learning \n",
    "rate as input. It initializes the model, trains the model, and returns the model.\n",
    "The main() function loads the data, trains the model, and evaluates the model.\n",
    "\n",
    "\"\"\"Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\"\"\"\n",
    "\n",
    "Ans: To optimize the performance of the gradient boosting model, we can experiment with different \n",
    "hyperparameters such as the learning rate, number of trees (n_estimators), and tree depth (max_depth). We \n",
    "can use either grid search or random search to find the best hyperparameters. Here's an example using \n",
    "scikit-learn's RandomizedSearchCV for random search:\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'learning_rate': uniform(0.001, 0.1),\n",
    "    'max_depth': randint(3, 6)\n",
    "}\n",
    "\n",
    "# Create the gradient boosting model\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(gbm, param_dist, cv=3, scoring='neg_mean_squared_error', \n",
    "n_iter=10, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Create and train the model with the best hyperparameters\n",
    "best_gbm = GradientBoostingRegressor(**best_params)\n",
    "best_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = best_gbm.predict(X_train)\n",
    "y_pred_test = best_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse_train = best_gbm.mse(y_train, y_pred_train)\n",
    "mse_test = best_gbm.mse(y_test, y_pred_test)\n",
    "r2_train = best_gbm.r_squared(y_train, y_pred_train)\n",
    "r2_test = best_gbm.r_squared(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train MSE:\", mse_train)\n",
    "print(\"Test MSE:\", mse_test)\n",
    "print(\"Train R-squared:\", r2_train)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "\n",
    "\n",
    "In this example, we define a parameter distribution using the randint and uniform functions from scipy.\n",
    "stats. The distributions specify the ranges for the hyperparameters n_estimators, learning_rate, and \n",
    "max_depth for the random search.\n",
    "\n",
    "We create an instance of RandomizedSearchCV with the gradient boosting model, the parameter distribution,\n",
    "and other parameters such as the number of cross-validation folds (cv=3), the scoring metric \n",
    "(neg_mean_squared_error), the number of iterations (n_iter=10), and the random seed (random_state).\n",
    "\n",
    "The RandomizedSearchCV performs a random search over the parameter distribution, sampling combinations of \n",
    "hyperparameters and evaluating them using cross-validation. After the search is complete, we can access\n",
    "the best hyperparameters using random_search.best_params_.\n",
    "\n",
    "Finally, we create and train the gradient boosting model with the best hyperparameters. We make \n",
    "predictions on the training and test sets and evaluate the model's performance using mean squared error \n",
    "(MSE) and R-squared metrics.\n",
    "\n",
    "You can adjust the parameter distribution, the number of iterations (n_iter), and other parameters to \n",
    "customize the random search. Additionally, you can use GridSearchCV instead of RandomizedSearchCV if you \n",
    "prefer an exhaustive search over the entire grid of parameter combinations.\n",
    "\n",
    "\n",
    "\"\"\"Q4. What is a weak learner in Gradient Boosting?\"\"\"\n",
    "Ans: \n",
    "A weak learner is a machine learning model that is only slightly better than random guessing. Weak \n",
    "learners are often used in ensemble learning algorithms, such as gradient boosting, to create a strong \n",
    "learner.\n",
    "\n",
    "In gradient boosting, weak learners are trained sequentially, with each learner learning to predict the \n",
    "residuals from the previous learner. The residuals are the errors made by the previous learner, and they \n",
    "represent the parts of the data that the previous learner was unable to predict. By iteratively training \n",
    "weak learners to predict the residuals, gradient boosting can create a strong learner that is able to \n",
    "predict the target variable very accurately.\n",
    "\n",
    "The following are some examples of weak learners:\n",
    "\n",
    "Decision trees\n",
    "Logistic regression\n",
    "Support vector machines\n",
    "Weak learners are typically simple models that are easy to train. They are also typically not very \n",
    "accurate on their own, but they can be very effective when used in ensemble learning algorithms.\n",
    "\n",
    "Gradient boosting is a powerful machine learning algorithm that can be used to solve a variety of problems.\n",
    "It is able to achieve very high accuracy by iteratively training weak learners to predict the residuals \n",
    "from previous learners.\n",
    "\n",
    "Here are some of the advantages of using weak learners in gradient boosting:\n",
    "\n",
    "Simple to train: Weak learners are typically simple models that are easy to train. This makes them a good \n",
    "choice for large datasets, where training time can be a major concern.\n",
    "Effective: Weak learners can be very effective when used in ensemble learning algorithms, such as gradient\n",
    "boosting. This is because they are able to learn the residuals from previous learners, which are the parts \n",
    "of the data that the previous learners were unable to predict.\n",
    "Versatile: Weak learners can be used to solve a variety of problems. This makes them a good choice for \n",
    "general-purpose machine learning applications.\n",
    "Here are some of the disadvantages of using weak learners in gradient boosting:\n",
    "\n",
    "Can be inaccurate: Weak learners are typically not very accurate on their own. This means that they can\n",
    "only be used effectively in ensemble learning algorithms.\n",
    "Can be unstable: Weak learners can be unstable, which means that they can be sensitive to small changes in\n",
    "the data. This can make it difficult to train a strong learner using gradient boosting.\n",
    "Can be computationally expensive: Training a large number of weak learners can be computationally \n",
    "expensive. This is especially true for large datasets.\n",
    "\n",
    "\"\"\"Q5. What is the intuition behind the Gradient Boosting algorithm?\"\"\"\n",
    "Ans: Gradient boosting is a machine learning algorithm that builds an ensemble of weak learners (e.g. \n",
    "decision trees) in an iterative fashion. The goal is to minimize the loss function by adding new models \n",
    "that correct the errors made by previous models.\n",
    "\n",
    "The intuition behind gradient boosting is that the errors made by a model can be used to train a new model\n",
    "that will reduce those errors. This process is repeated until the desired level of accuracy is achieved.\n",
    "\n",
    "Gradient boosting is a powerful algorithm that can be used for both classification and regression tasks. \n",
    "It is often used in conjunction with other machine learning algorithms, such as decision trees and random \n",
    "forests.\n",
    "\n",
    "Here is a simple example of how gradient boosting works:\n",
    "\n",
    "Start with a simple model, such as a decision tree.\n",
    "Calculate the error of the model.\n",
    "Train a new model to correct the errors made by the first model.\n",
    "Repeat steps 2 and 3 until the desired level of accuracy is achieved.\n",
    "Gradient boosting is a versatile algorithm that can be used for a variety of tasks. It is a powerful tool \n",
    "that can be used to improve the accuracy of machine learning models.\n",
    "\n",
    "Here are some of the advantages of gradient boosting:\n",
    "\n",
    "It can be used for both classification and regression tasks.\n",
    "It is a very accurate algorithm.\n",
    "It is relatively easy to implement.\n",
    "Here are some of the disadvantages of gradient boosting:\n",
    "\n",
    "It can be computationally expensive to train.\n",
    "It can be sensitive to the choice of hyperparameters.\n",
    "It can be prone to overfitting.\n",
    "Overall, gradient boosting is a powerful and versatile machine learning algorithm. It can be used for a\n",
    "variety of tasks and can achieve high accuracy. However, it is important to be aware of the potential \n",
    "disadvantages of the algorithm, such as its computational cost and its sensitivity to hyperparameters.\n",
    "\n",
    "\"\"\"Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\"\"\"\n",
    "Ans: The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Heres a \n",
    "step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "Initialization: The ensemble starts with an initial prediction, which can be a simple model such as the \n",
    "mean of the target variable. This initial prediction serves as the starting point for subsequent \n",
    "iterations.\n",
    "\n",
    "Compute Residuals: The algorithm calculates the residuals by taking the differences between the target \n",
    "variable and the current predictions of the ensemble model. These residuals represent the errors or \n",
    "discrepancies that the ensemble has not yet captured.\n",
    "\n",
    "Fit Weak Learner: A weak learner, typically a decision tree with limited depth or a shallow neural network,\n",
    "is trained on the data. The weak learner is trained to predict the residuals instead of the target variable.\n",
    "It learns to capture the patterns and relationships in the data that are not accounted for by the current\n",
    "ensemble model.\n",
    "\n",
    "Compute Learning Rate: The learning rate determines the contribution of each weak learner to the ensemble. \n",
    "It scales the predictions made by the weak learner before they are added to the ensemble. A lower learning\n",
    "rate makes the ensemble more conservative, while a higher learning rate allows each weak learner to have a\n",
    "larger impact.\n",
    "\n",
    "Update Ensemble Predictions: The predictions of the weak learner are combined with the predictions made by\n",
    "the current ensemble model. The combined predictions are weighted by the learning rate, and the updated \n",
    "predictions become the new target variable for the next iteration.\n",
    "\n",
    "Iterative Process: Steps 2-5 are repeated for a specified number of iterations or until a stopping \n",
    "criterion is met. In each iteration, a new weak learner is added, trained on the residuals, and its \n",
    "predictions are combined with the existing ensemble predictions.\n",
    "\n",
    "Final Ensemble Prediction: Once all iterations are completed, the final prediction of the ensemble is \n",
    "obtained by summing up the predictions made by all the weak learners, each multiplied by its learning rate.\n",
    "The ensemble prediction represents the combined knowledge and predictive power of all the weak learners.\n",
    "\n",
    "By iteratively adding weak learners that focus on capturing the errors or residuals of the previous \n",
    "models, the Gradient Boosting algorithm builds an ensemble that continually improves its predictive \n",
    "performance. Each weak learner contributes its specialized knowledge to the ensemble, allowing the model \n",
    "to learn complex relationships and make accurate predictions. The algorithm determines the weights and \n",
    "learning rates of the weak learners based on the gradients of the loss function, optimizing the ensemble's\n",
    "overall performance.\n",
    "\n",
    "\"\"\"Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\"\"\"\n",
    "Ans: Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several key \n",
    "steps. Here is a high-level overview of those steps:\n",
    "\n",
    "Loss Function Selection: Choose an appropriate loss function that measures the difference between the \n",
    "predicted and true values. For regression problems, the mean squared error (MSE) is commonly used, while \n",
    "for classification problems, the log loss or exponential loss functions are often employed.\n",
    "\n",
    "Initialization: Initialize the ensemble model by setting an initial prediction. For regression, this can \n",
    "be the mean of the target variable, while for classification, it can be the log-odds or class proportions.\n",
    "\n",
    "Compute Residuals: Calculate the residuals by taking the differences between the true values and the \n",
    "current predictions of the ensemble model. These residuals represent the errors or discrepancies that the \n",
    "ensemble has not yet captured.\n",
    "\n",
    "Fit Weak Learner: Train a weak learner (e.g., decision tree, neural network) to predict the residuals. The\n",
    "weak learner is trained to capture the patterns and relationships in the data that are not accounted for\n",
    "by the current ensemble model.\n",
    "\n",
    "Compute Weights: Determine the weight or coefficient to assign to the weak learner's predictions. This\n",
    "weight is calculated by minimizing the loss function with respect to the residuals, typically using \n",
    "techniques like gradient descent.\n",
    "\n",
    "Update Ensemble Predictions: Combine the predictions of the weak learner with the current ensemble \n",
    "predictions, weighted by the coefficient obtained in the previous step. This update step incorporates the\n",
    "information from the weak learner into the ensemble model.\n",
    "\n",
    "Repeat Steps 3-6: Iterate the process by recalculating the residuals using the updated ensemble \n",
    "predictions, fitting a new weak learner to predict the residuals, computing the weights, and updating the \n",
    "ensemble predictions. Repeat these steps until a predefined stopping criterion is met (e.g., a maximum \n",
    "number of iterations or reaching a desired level of performance).\n",
    "\n",
    "Final Ensemble Prediction: Once the iterations are complete, the final prediction of the ensemble is \n",
    "obtained by summing up the predictions made by all the weak learners, each multiplied by its \n",
    "corresponding weight. This ensemble prediction represents the combined knowledge and predictive power of \n",
    "all the weak learners.\n",
    "\n",
    "These steps outline the general mathematical intuition behind the Gradient Boosting algorithm. The \n",
    "specifics of the optimization techniques, such as gradient descent, may vary depending on the particular \n",
    "implementation or variant of Gradient Boosting being used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
